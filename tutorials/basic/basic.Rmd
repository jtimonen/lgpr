---
title: "Basic usage of the lgpr package"
author: "Juho Timonen"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Basic usage of the lgpr package}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#")
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
```

This vignette demonstrates how to fit an additive Gaussian process (GP) model to a longitudinal data set and perform covariate relevance assessment using the `lgpr` package.

```{r load}
require("lgpr")
require("ggplot2")
```

```{r simulate, include = FALSE, eval = FALSE}
set.seed(1932)
sim <- simulate_data(N            = 12,                     
                     t_data       = seq(12, 96, by = 12),
                     covariates   = c(    0,2), # covariate types (2 = cat)
                     lengthscales = c(16,24,1,16), # true effect lengthscales
                     relevances   = c(0,1,1,1), # true covariate relevances
                     names        = c("diseaseAge", "sex"),
                     t_jitter     = 0,  # jitter in time points
                     snr          = 3)    # signal-to-noise ratio
dat <- sim@data
dat$y <- 100*(dat$y + 5)
a <- as.numeric(dat$id)
dat$id <- as.factor(formatC(a, width = 2, flag = "0"))
dat$group <- as.factor(as.numeric(!is.nan(dat$diseaseAge)))
dat$sex <- as.factor(c("Male", "Female")[as.numeric(dat$sex)])
dat$group <- as.factor(c("Control", "Case")[as.numeric(dat$group)])
plot_sim(sim)
plot_sim(sim, comp_idx = 2)
```

## Visualizing longitudinal data
In this tutorial we use simulated `testdata_002`, which is included in the `lgpr` package. It consists of 12 individuals and 8 time points for each. 

``````{r head}
str(testdata_002)
```

The `plot_data()` function
can be used to visualize the data in many ways.

``````{r pd1, fig.width=7.2, fig.height=4.8}
plot_data(testdata_002, facet_by = "id", color_by = "sex") + xlab('Age (months)')
```

Coloring according to the disease-related age (`diseaseAge`) shows that individuals 01-06 are cases and 07-12 are controls. The observed disease onset is at around 60 months for each case individual.
``````{r pd2, fig.width=7.2, fig.height=4.8}
plot_data(testdata_002, facet_by = "id", color_by = "diseaseAge") +
  scale_color_gradient2() + xlab('Age (months)')
```


## Creating and fitting a model

Detailed instructions and the syntax for creating different kinds of models is found in the package documentation and the *Defining a model* tutorial.

Here we define a model with four components and fit it. The `gp(age)` component
is a shared age effect, `zs(sex)*gp(age)` is a sex-specific deviation from it,
`gp_vm(diseaseAge)` is a non-stationary variance-masked disease effect, and
`zs(group)` is a static offset component between the two groups (Case/Control).

```{r fit, cache=TRUE}
fit <- lgp(formula = y ~ gp(age) + zs(id)*gp(age) + zs(sex)*gp(age) + gp_vm(diseaseAge) + zs(group),
           data     = testdata_002,
           prior    = list(wrp = igam(14,5)),
           iter     = 1000,
           chains   = 4,
           refresh  = 250)
```

## Studying the model and fit
Printing the model object gives info about the model components, variables
and parameter priors. In this example we did not specify the `prior` argument when calling `lgp()` so default priors were used.

```{r study1}
print(fit@model)
```

There are four magnitude parameters (alpha), one for each component, and three lengthscale parameters (ell), since the `zs(group)`component does not have a lengthscale parameter. 
 
**Note:** the continuous covariates and the response variable have been normalized to zero mean and unit variance, and the inference of the lengthscale, magnitude, and noise parameters is done on that scale.

Printing the fit object gives info about the posterior distribution of the
parameters, MCMC settings and convergence diagnostics.

```{r study2}
print(fit)
```

Distribution of the parameter draws can be visualized.

```{r study3, fig.width = 6, fig.height = 3.5}
plot_draws(fit, type = 'dens')
```

## Assessing component relevances
Here we compute the average relevance of each component. We see that `id` and `group` have a very low relevance.

```{r study4, fig.width = 6, fig.height = 3.5}
Relevance <- relevances(fit, reduce = mean)
data.frame(Relevance)
```

Components can be selected using a threshold for the proportion of total explained variance. We see that `id` and `group` are not selected.
```{r study5, fig.width = 6, fig.height = 3.5}
select(fit, threshold = 0.95)
```

In order to avoid having to set a fixed `threshold` for selection, we can integrate over a threshold density on the [0,1] interval, and obtain probabilistic selection. Here we define the density to be `stats::dbeta(x, 20, 2)`, which has most mass between $0.9$ and $1.0$.

```{r study6, fig.width = 6, fig.height = 3.5}
threshold_density <- function(x) {stats::dbeta(x, 20, 2)}
s <- select.integrate(fit, p = threshold_density)
print(s$expected)
```

The above table shows for each component the expectation value of being selected.

## Out-of-sample predictions

We visualize the predictive distribution of the model, using posterior 
mean hyperparameters (`reduce = mean`). The predictive mean ($\pm 2 \times$ standard deviation) are computed at 120 points for each of the 12 individuals (1440 total prediction points created using `new_x()`) and visualized against the data.

```{r vis3, fig.width=7, fig.height=4}
t <- seq(1, 120, by = 1)
x_pred <- new_x(testdata_002, t, x_ns = "diseaseAge")
p <- pred(fit, x_pred, reduce = mean)
plot_pred(fit, x = x_pred, pred = p)
```

## Visualizing the inferred covariate effects

Also the posterior distribution of each component can be visualized

```{r vis4, fig.width=7, fig.height=7}
plot_components(fit, 
                x = x_pred, 
                pred = p,
                color_by = c(NA, NA, "sex", "group", "group", "group"),
                ylim = c(-3,2),
                ncol = 2)
```

**Note:** the `plot_pred()` function scales the predictions back to the original data scale, whereas the `plot_components()` function plots the inferred functions on the (normalized) inference scale.

**Note:** the `plot_pred()` and `plot_components()` functions could be used without the `x` and `pred` arguments, too, in which case computing out-of-sample predictions would not be needed. The predictive and component posteriors would be shown only at the data points.

## Computing environment

```{r sess}
sessionInfo()
```
