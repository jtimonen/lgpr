---
title: "Generating artificial data using the lgpr package"
author: "Juho Timonen"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
#  pdf_document:
#    toc: true     
vignette: >
  %\VignetteIndexEntry{Generating artificial data using the lgpr package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This document demonstrates how to use the `lgpr` package to simulate longitudinal data sets for testing purposes. The function that simulates artificial data sets is called `simulateData`. Data sets created with it can be visualized using the functions `plotSimData` and `plotSimDataComponents`.  

```{r}
set.seed(12)
library(lgpr)
```

## Generating a data set

### A minimal example
The `simulateData` function takes as its basic input the number of individuals `N` and number of measurement times `k`.
```{r}
# Simulate data set with 12 individuals and 5 measurement points for each
#dat <- simulateData(N = 12, k = 5, grid = 30)
```

Its output consists of the following objects: 
```{r}
#print(names(dat))
```

The `data` object is the actual data matrix, and its columns are the covariates and the response variable `y`. It can be used as such as input to `lgp`. 
```{r}
# Look at the generated data
#head(dat$data, n = 7L)
```

In this example, we did not specify the data covariates, and therefore the generated data consisted of only two covariates: `id` and `age`. These covariates will always be generated by `simulateData`, as they are mandatory to have in a data that is passed as input to the `lgp` function. The signal $f$ is generated as a sum of $D$ additive components, so that
$$
f = f_1 + \cdots + f_D
$$

These components are stored in the `components` object along with the data-generating function values also outside the measurement points, and some other things needed for visualizations. 
```{r}
## Look at the generated components
#head(dat$components)
```
Because we did not specify the data covariates, created signal $f$ consisted only of an individual-specific age component $f_1$ (the `id.age` column) and a shared age component $f_2$ (the `age` column) which is same for all individuals. 


### Specifying the covariates and components

These covariates `id` and `age` will always be generated by `simulateData`, as they are mandatory to have in a data that is passed as input to the `lgp` function. In addition, it function allows the user to define other covariates that can have four different types. These have the following integer encodings:

 - 0 = disease-related age (see  section *Simulating data with healthy and diseased individuals*)
 - 1 = other continuous covariate
 - 2 = a categorical covariate that interacts with age
 - 3 = a categorical covariate that acts as a group offset
 
that are used to specify the covariate types in the `covariates` argument. By default the `covariates` arguments has length zero, and adding one covariate will always add one additive component to the data. The user can also give a vector `relevances` as input, in which case the variances of the corresponding components will be scaled to these values. Length of `relevances` must be `2 + length(covariates)`, because the first two values are relevances of the individual-specific and shared age components, respectively. The optional `names` argument can be used to define names for the covariates. The `n_cat` argument can be specified to determine the number of categories for each categorical covariate, and its length must be equal to the number of 2's and 3's in the `covariates` vector.


```{r}
# Simulate data with two categorical covariates that interact with age and one
# categorical covariate that acts as a group offset.
#dat <- simulateData(N = 12, 
#                    k = 5, 
#                    covariates = c(2,2,3),
#                    names = c("sex", "location", "batch"),
#                    relevances = c(0.5,4,1,1,1),
#                    n_cat = c(2,3,2),
#                    grid = 30)
```


## Visualizing a data set

### Plotting the measurements and the underlying signal
The below example demonstrates the use of `plotSimData`.
```{r, fig.show='hold', fig.width = 7, fig.height = 4.5, fig.align = "center"}
# Plot the data that was generated in the previous block
#plotSimData(dat, N_COLS = 4)
```

The dotted line depicts the underlying signal that generates the data, and the black dots are noisy measurements. The underlying signal is drawn from an additive GP prior and the `grid` argument is relevant for visualization purposes since it determines the number of points where the function values are computed. **NOTE:** using larger values of `grid` will produce smoother visualizations but slow down the computations involved in the data generation, and increase amount of memory needed for storing the data.


### Visualizing the additive components
Plotting the data and the underlying signal
To see the different components, one can use the `plotSimDataComponents` function. As can be seen below, it plots the five additive components (`id.age`, `age`, `sex.age`, `location.age`, `batch`), their sum `f`, the noise and their sum `y_tot = f + noise`. The `highlight` argument can be given to specify which categorical covariates should be hightlighted so that the effect for different groups is shown in different color. 

```{r, fig.show='hold', fig.width = 7, fig.height = 3.5, fig.align = "center"}
# Plot the components
#plotSimDataComponents(dat, 
#                         highlight = c("sex", "location", "batch"),
#                         legendPosition = NA,
#                         N_COLS = 4)
```

## Defining the amount and type of measurement noise

The `simulateData` function has an argument `noiseType`, which can be

 - `"Gaussian"` (default)
 - `"Poisson"` or
 - `"NB"`

### Gaussian noise
In the case of Gaussian noise, the measurement noise is generated independently of the signal $f$ and is just added on top of it. The amount of noise can specified relative to the signal variance by using the `snr` argument, which defines the desired signal-to-noise ratio. The noise variance will be scaled so that
$$
\text{snr} = \frac{\text{Var}[\bf{f}]}{\text{Var}[\bf{y_n}]},
$$
where $\bf{f}$ is the vector of the generated signal values at the grid points and $\bf{y_n}$ is the vector of the measurement noise values at the grid points. The fact that this scaling is done for the values at the grid points, and not measurement points, means that the signal-to-noise ratio can be a bit different if computed over the measurement points, though its expectation is still the given value `snr`.

```{r}
# Simulate data with Gaussian noise so that the signal-to-noise ratio is 4
#dat <- simulateData(N = 12, 
#                    k = 5, 
#                    covariates = c(2,2,3),
#                    names = c("sex", "location", "batch"),
#                    relevances = c(1,1,1,0,0),
#                    n_cat = c(2,3,2),
#                    grid = 30,
#                    noiseType = "Gaussian", 
#                    snr = 4)
```

To see the variances of different components and noise in the data, one can run the following command:
```{r}
#printSimDataStatistics(dat)
```

Notice that we set `relevances = c(1,1,1,0,0)`, in which case the first three components (`id.age`, `age` and `sex.age`) are scaled to have equal variance and the remaining two (`location.age` and `batch`) have zero variance, i.e. the covariates `location` and `batch` are not relevant. 

### Non-Gaussian noise
The possible non-Gaussian noise types are Poisson and negative binomial (NB). These distributions have the non-negative integers as their support, and therefore the generated measurements will be integers. In the Poisson case, the measurements $y_i$ are generated so that
$$
y_i \sim \text{Poisson}(f_i),
$$
where $f_i$ is the value of the signal $f$ at the $i$th time point. The Poisson distribution has the property that $\text{E}[y_i] = \text{Var}[y_i] = f_i$. In the negative binomial case, 
$$
y_i \sim \text{NegBinomial}(f_i, \phi),
$$
where $\phi$ is a precision parameter, so that $\text{Var}[y_i] = f_i + \frac{f_i^2}{\phi}$ i.e. larger values of $\phi$ give less noisy data. Note that the amount of Poisson and negative binomial noise will depend on the signal values. The `f_range` argument determines the maximum and minimum that the generated signal will eventually be scaled to have.

```{r, fig.show='hold', fig.width = 7, fig.height = 7/4, fig.align = "center"}
# Simulate data with negative binomial noise
#dat <- simulateData(N = 4, 
#                    k = 10, 
#                    grid = 30,
#                    covariates = c(2),
#                    noiseType = "NB", 
#                    phi = 10,
#                    f_range = c(1e3,1e4)
#)
#
#plotSimData(dat, N_COLS = 4,  YLIM = c(0, 2.5*1e4) )
```

## Simulating data with healthy and diseased individuals

The disease covariate can be included by including a zero in the `covariates` vector. This will make half of the individuals cases, and randomly draw a disease onset for the cases. By default, the disease effect is generated equally strong for each of the diseased individuals. Howere, we demonstrate generating data where some diseased individuals do not have an effect, using the `n_affected` argument.
```{r, fig.show='hold', fig.width = 7, fig.height = 7, fig.align = "center"}
# Simulate data with half of the individuals cases and half of them controls
#dat <- simulateData(N = 16, 
#                    k = 6, 
#                    covariates = c(0,2),
#                    names = c("diseaseAge", "location"),
#                    grid = 60,
#                    n_affected = 6)
#plotSimData(dat, N_COLS = 4)
```

In the above figure, the red background depicts the time when the individual actually has the disease. At the changepoint from white to red, the underlying signal experiences drastic changes for the first 6 individuals but the remaining 2 diseased individuals have no special effect. The red vertical line on the other hand shows the first measurement when the disease can be diagnosed.
