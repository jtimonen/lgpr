---
title: "Mathematical description of lgpr models"
author: Juho Timonen
date: 9th August 2021
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of lgpr models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.json
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
library(lgpr)
```

This vignette describes mathematically the statistical models of `lgpr`. We
study the different arguments of the `lgp()` or `create_model()` modeling
functions and what parts of the probabilistic model they customize.

## Bayesian GP regression
The models in `lgpr` are models for the conditional distribution 
$$
p(y \mid f(\textbf{x}), \theta),
$$
of response variable $y$ given covariates $\textbf{x}$, where $\theta_{\text{obs}}$ is a possible parameter of the observation model
(like the magnitude of observation noise). The function $f$ has a GP prior 
$$
f \sim \mathcal{GP}(0, k\left(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}})\right),
$$

with covariance function $k(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}})$
that has hyperparameters $\theta_{\text{GP}}$. In addition to the GP prior for $f$, there is a parameter prior distribution $p(\theta)$ for $\theta = \left\{ \theta_{\text{GP}}, \theta_{\text{obs}} \right\}$. Given $N$ observations $\mathcal{D} = \{y_n, \textbf{x}_n\}_{n=1}^N$
the probabilistic models in `lgpr` have the form
\begin{align}
p\left(\theta, \textbf{f}\right) &= p\left(\textbf{f} \mid \theta\right) \cdot p(\theta) & \text{(prior)} \\
p(\textbf{y} \mid \textbf{f}, \theta) &= \prod_{n=1}^N p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}}) & \text{(likelihood)},
\end{align}
where $\textbf{f} = \left[ f(\textbf{x}_1), \ldots, f(\textbf{x}_N) \right]^{\top}$, $\textbf{y} = \left[y_1, \ldots, y_N\right]^{\top}$. The 
parameter prior density $p(\theta)$ is the product of the prior densities of each parameter, and the GP prior means that the prior for $\textbf{f}$ is the multivariate normal
\begin{equation}
p\left(\textbf{f} \mid \theta\right) = \mathcal{N}\left(\textbf{f} \mid \textbf{0}, \textbf{K} \right),
 \end{equation}
 where the $N \times N$ matrix $\textbf{K}$ has entries $\{ \textbf{K} \}_{in} = k(\textbf{x}_i, \textbf{x}_n \mid \theta_{\text{GP}})$.

## Connection between lgpr arguments and different model parts

The below table shows which parts of the above mathematical description
are affected by which arguments to `lgp()` or `create_model()`. You can
read more about them in the documentation of said functions.

| Argument      | Affected model part                                |
| ------------- |----------------------------------------------------|
| `formula`     | $k(\textbf{x}, \textbf{x}')$                       |
| `data`        | $\mathcal{D}$                                      |
| `likelihood`  | $p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}})$ |
| `prior`       | $p(\theta)$                                        |
| `c_hat`       | $p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}})$ |
| `num_trials`  | $\mathcal{D}$                                      |
| `options`     | $k(\textbf{x}, \textbf{x}')$                       |


## Observation models
The terms **observation  model** and **likelihood** are used to refer to the same formula, i.e. $p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}})$, though the former means it as a function of $\textbf{y}$ and the latter as a function of $\theta$. There
are currently five observation models available and they all involve
an inverse link function transformation
$$
h_n = g^{-1}\left(  f(\textbf{x}_n)+ \hat{c}_n \right)
$$
where $g$ is determined by the `likelihood` argument and $\hat{c}_n$ by the
`c_hat` argument. The below table shows what the link function is in different
cases, and what parameter the corresponding observation model has.

| `likelihood`  | Link function $g$ | Parameter $\theta_{\text{obs}}$|
| ------------- |-------------------|--------------------------------|
| `gaussian`    | identity          | $\sigma$                       |
| `poisson`     | logarithm         | -                              |
| `nb`          | logarithm         | $\phi$                         |
| `binomial`    | logit             | -                              |
| `bb`          | logit             | $\gamma$                       |


* In the Gaussian observation model (`likelihood="gaussian"`),
\begin{equation}
p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}}) = \mathcal{N}(y_n \mid h_n, \sigma^2)
\end{equation}
$\theta_{\text{obs}}=\sigma$ is a noise magnitude parameter.


* The **Poisson** observation model (`likelihood="poisson"`) for count data is $y_n \sim \text{Poisson}\left(\lambda_n \right)$, where the rate is 
$\lambda_n = h_n$. 

* In the **negative binomial** (`likelihood="nb"`) model, $\lambda_n$ is gamma-distributed with parameters
\begin{equation}
\begin{cases}
\text{shape} &= \phi \\
\text{scale} &= \frac{\phi}{h_n}
\end{cases},
\end{equation}
and $\phi > 0$ controls overdispersion so that $\phi \rightarrow \infty$ corresponds to the Poisson model.

* When selecting the binomial or beta-binomial observation model for count data, the number of trials $\eta_n$, for each $n=1, \ldots, N$ has to be supplied using the `num_trials` argument. The **binomial** model (`likelihood="binomial"`) is $y_n \sim \text{Binomial}(h_n, \eta_n)$, where the success probability $\rho_n = h_n$. 

* In the **beta-binomial** model (`likelihood="bb"`), $\rho_i$ is random so that
\begin{equation}
\rho_n \sim \text{Beta}\left(h_n \cdot \frac{1 - \gamma}{\gamma}, \  (1-h_n) \cdot \frac{1 - \gamma}{\gamma}\right),
\end{equation}
and the parameter $\gamma \in  [0, 1]$ controls overdispersion so that $\gamma \rightarrow 0$ corresponds to the binomial model. 

When using the Gaussian observation model with `sample_f=TRUE` the continuous response $y$ is normalized to unit variance and zero mean, and $\hat{c}_n = 0$ for all $n$ is set. In this case the `c_hat` argument has no effect. With 
`sample_f = TRUE`, sensible defaults are used. See the documentation
of the `c_hat` argument of `lgp()` for exact details, and the [Model inference] section below for more information about the `sample_f` argument.

## Additive GP regression

The GP models of `lgpr` are additive, so that
\begin{equation}
 k(\textbf{x}, \textbf{x}')  = \sum_{j=1}^J k_j(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}}).
\end{equation}
This is equivalent to saying that we have $f = f^{(1)} + \ldots + f^{(J)}$
modeled so that each component $j = 1, \ldots, J$ has a GP prior
\begin{equation}
f^{(j)} \sim \mathcal{GP}\left(0, k_j(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}}) \right),
\end{equation}
independently from other components. The number of components $J$ is equal
to the number of terms that you specify in your `formula`. Each formula
term defines what the corresponding kernel $k_j$ will be like, and what
covariates and parameters it depends on. 

See @timonen2021 for information about the different kernel functions and
the "Basic usage" tutorial for information about how they are created based
of the supplied `formula`.

## Model inference

After the model is defined, `lgpr` uses the MCMC methods of Stan to obtain
draws from the joint posterior $p\left(\theta, \textbf{f} \mid \mathcal{D}\right)$ or the marginal posterior of parameters, i.e. 
$p\left(\theta \mid \mathcal{D}\right)$. Which one of these is done is determined by the `sample_f` argument of `lgp()` or `create_model()`.

### With sample_f = TRUE
This option is always possible but not recommended if `likelihood == "gaussian"`. The joint posterior that is sampled from is
\begin{equation}
p\left(\theta, \textbf{f} \mid \mathcal{D}\right) \propto p\left(\theta, \textbf{f}\right) \cdot p(\textbf{y} \mid \textbf{f}, \theta) \\
\end{equation}
and sampling requires evaluating the right-hand side and its gradient thousands of times. 

### With sample_f = FALSE
This option is only possible, and is automatically selected by default,
if `likelihood == "gaussian"`. This is because $p\left(\theta \mid \mathcal{D}\right)$ is analytically available only in this case. The
distribution that is sampled from is
\begin{equation}
p\left(\theta \mid \mathcal{D}\right) \propto p\left(\theta\right) \cdot p(\textbf{y} \mid \theta) \\
\end{equation}
and now sampling requires repeatedly evaluating the right-hand side of this equation and its gradient. This analytical marginalization reduces the MCMC dimension by $N$ and likely improves sampling efficiency. The likelihood is
\begin{equation}
p\left(\textbf{y} \mid \theta\right) = \mathcal{N}\left(\textbf{y} \mid \textbf{0}, \textbf{K} + \sigma^2 \textbf{I} \right).
\end{equation}

## References

