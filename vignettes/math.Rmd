---
title: "Mathematical description of lgpr models"
author: Juho Timonen
output:
  rmarkdown::html_vignette:
    toc: true
vignette: >
  %\VignetteIndexEntry{Mathematical description of lgpr models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
library(lgpr)
```

This vignette describes mathematically the statistical models of `lgpr`, and 
shows which arguments of the `lgp()` modeling function customize which 
parts of the probabilistic model.

## Bayesian GP regression
The models in `lgpr` are models for the conditional distribution 
$$
p(y \mid f(\textbf{x}), \theta),
$$
of response variable $y$ given covariates $\textbf{x}$, where $\theta_{\text{obs}}$ is a possible parameter of the observation model
(like the magnitude of observation noise). The function $f$ has a GP prior 
$$
f \sim \mathcal{GP}(0, k\left(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}})\right),
$$

with covariance function $k(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}})$
that has hyperparameters $\theta_{\text{GP}}$. In addition to the GP prior for $f$, there is a parameter prior distribution $p(\theta)$ for $\theta = \left\{ \theta_{\text{GP}}, \theta_{\text{obs}} \right\}$. Given $N$ observations $\mathcal{D} = \{y_n, \textbf{x}_n\}_{n=1}^N$
the probabilistic models in `lgpr` have the form
\begin{align}
p\left(\theta, \textbf{f}\right) &= p\left(\textbf{f} \mid \theta\right) \cdot p(\theta) & \text{(prior)} \\
p(\textbf{y} \mid \textbf{f}, \theta) &= \prod_{n=1}^N p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}}) & \text{(likelihood)},
\end{align}
where $\textbf{f} = \left[ f(\textbf{x}_1), \ldots, f(\textbf{x}_N) \right]^{\top}$, $\textbf{y} = \left[y_1, \ldots, y_N\right]^{\top}$. The 
parameter prior density $p(\theta)$ is the product of the prior densities of each parameter, and the GP prior means that the prior for $\textbf{f}$ is the multivariate normal
\begin{equation}
p\left(\textbf{f} \mid \theta\right) = \mathcal{N}\left(\textbf{f} \mid \textbf{0}, \textbf{K} \right),
 \end{equation}
 where the $N \times N$ matrix $\textbf{K}$ has entries $\{ \textbf{K} \}_{in} = k(\textbf{x}_i, \textbf{x}_n \mid \theta_{\text{GP}})$.

## Connection between lgp() arguments and different model parts

The below table shows which parts of the above mathematical description
are affected by which arguments to `lgp()` or `create_model()`. You can
read more about them in the documentation of said functions.

| Argument      | Affected model part                                |
| ------------- |----------------------------------------------------|
| `formula`     | $k(\textbf{x}, \textbf{x}')$                       |
| `data`        | $\mathcal{D}$                                      |
| `likelihood`  | $p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}})$ |
| `prior`       | $p(\theta)$                                        |
| `c_hat`       | $p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}})$ |
| `num_trials`  | $\mathcal{D}$                                      |
| `options`     | $k(\textbf{x}, \textbf{x}')$                       |


## Observation models
There are currently five observation models available.

* In the Gaussian observation model (`likelihood="gaussian"`),
\begin{equation}
p(y_n \mid f(\textbf{x}_n), \theta_{\text{obs}}) = \mathcal{N}(y_n \mid f(\textbf{x}_n), \sigma^2)
\end{equation}
$\theta_{\text{obs}}=\sigma$ is a noise magnitude parameter.

* Poisson (`likelihood="poisson"`).
* Negative binomial (`likelihood="nb"`).
* Binomial (`likelihood="binomial"`).
* Beta-binomial (`likelihood="bb"`).

## Additive GP regression

The GP models of `lgpr` are additive, so that
\begin{equation}
 k(\textbf{x}, \textbf{x}')  = \sum_{j=1}^J k_j(\textbf{x}, \textbf{x} \mid \theta_{\text{GP}}â€™).
\end{equation}
This is equivalent to saying that we have $f = f^{(1)} + \ldots + f^{(J)}$
modeled so that each component $j = 1, \ldots, J$ has a GP prior
\begin{equation}
f^{(j)} \sim \mathcal{GP}\left(0, k_j(\textbf{x}, \textbf{x}' \mid \theta_{\text{GP}}) \right),
\end{equation}
independently from other components. The number of components $J$ is equal
to the number of terms that you specify in your `formula`. Each formula
term defines what the corresponding kernel $k_j$ will be like, and what
covariates and parameters it depends on.

## Model inference

After the model is defined, `lgpr` uses the MCMC methods of Stan to obtain
draws from the joint posterior $p\left(\theta, \textbf{f} \mid \mathcal{D}\right)$ or the marginal posterior of parameters, i.e. 
$p\left(\theta \mid \mathcal{D}\right)$. Which one of these is done is determined by the `sample_f` argument of `lgp()` or `create_model()`.

### With sample_f = TRUE
This option is always possible but not recommended if `likelihood == "gaussian"`. The joint posterior that is sampled from is
\begin{equation}
p\left(\theta, \textbf{f} \mid \mathcal{D}\right) \propto p\left(\theta, \textbf{f}\right) \cdot p(\textbf{y} \mid \textbf{f}, \theta) \\
\end{equation}
and sampling requires evaluating the right-hand side and its gradient thousands of times. 

### With sample_f = FALSE
This option is only possible, and is automatically selected by default,
if `likelihood == "gaussian"`. This is because $p\left(\theta \mid \mathcal{D}\right)$ is analytically available only in this case. The
distribution that is sampled from is
\begin{equation}
p\left(\theta \mid \mathcal{D}\right) \propto p\left(\theta\right) \cdot p(\textbf{y} \mid \theta) \\
\end{equation}
and now sampling requires repeatedly evaluating the right-hand side of this equation and its gradient. This analytical marginalization reduces the MCMC dimension by $N$ and likely improves sampling efficiency. The likelihood is
\begin{equation}
p\left(\textbf{y} \mid \theta\right) = \mathcal{N}\left(\textbf{y} \mid \textbf{0}, \textbf{K} + \sigma^2 \textbf{I} \right).
\end{equation}



