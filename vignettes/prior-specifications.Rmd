---
title: "Specifying priors in the lgpr package"
author: "Juho Timonen"
date: "`r Sys.Date()`"
output: 
  rmarkdown::html_vignette:
    toc: true
#  pdf_document:
#    toc: true     
vignette: >
  %\VignetteIndexEntry{Specifying priors in the lgpr package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```
  
## The sampled parameters
  
All parameters that are sampled have default priors which can be replaced by user-supplied priors. The sampled parameters can be
  
* the magnitude parameters $\alpha$ of different components
* the lengthscale parameters $\rho$ of different components
* the steepness parameter $\tau$ of the input warping function for a nonstationary component
* noise standard deviation $\sigma_n$ for Gaussian likelihood model
* the uncertain disease onset $t_d$ for diseased individuals (only the default uniform prior allowed)

## Defining the priors
### Default priors
In the simplest case, the `prior` argument for the `lgp` function can be ignored, in which case default priors are used. [insert here what they are]

### User-defined priors for each parameter type

The prior can be supplied as a list containing the priors for each parameter type separately. The elements of the list must be named `magnitude`, `lengthscale`, `noise` or `steepness`, and be themselves lists, that contain the type of the prior distribution, as well as its hyperparameters.

```r
my_prior <- list()
my_prior$magnitude    <- list(type="student_t", nu=1)
my_prior$lengthscale  <- list(type="inv_gamma", alpha=3, beta=3)
my_prior$noise        <- list(type="normal", mu=0, sigma=1)
my_prior$steepness    <- list(type="normal", mu=10, sigma=5)
```

If any of the possible fields of the list is missing, the corresponding default prior will be used.

### Different user-defined priors for each model component type
In some cases, the user may want to use a different prior for example for the magnitudes of different components. This can be done by defining the `magnitude` field as a list, that can contain six possible fields, which are demonsrated in the below example:

```r
my_prior$magnitude$idAge         <- list(type="student_t", nu=4)
my_prior$magnitude$sharedAge     <- list(type="student_t", nu=20)
my_prior$magnitude$diseaseAge    <- list(type="student_t", nu=20)
my_prior$magnitude$continuous    <- list(type="normal", mu=0, sigma=1)
my_prior$magnitude$categorical   <- list(type="uniform")
my_prior$magnitude$offset        <- list(type="gamma", alpha=5, beta=2)
```

This can be done also for the fields `lengthscale`, but with the exception that the field `offset` must be skipped, since offset components do not have a lengthscale parameter. If any of the possible fields are missing, the corresponding default prior will be used.

## Prior distributions
The possible types of prior distributions are
  
1. Uniform (`uniform`)
2. Normal (`normal`)
3. Student-t (`student-t`)
4. Gamma (`gamma`)
5. Inverse-Gamma (`inv-gamma`)
6. Log-normal (`log-normal`)
7. Beta (`beta`)
  
In addition, all kernel parameters $\alpha, \rho, \sigma_n, \tau$ are constrained to be non-negative, so only the positive part of the defined prior will matter. The hyperparameters of the prior (for example the mean and standard deviation of the normal prior) can be given by the user. Below, we will look at each of the different distributions in detail. In the graphics::plots $x$ is a general parameter, which could be any of the ones listed above. The red curves represent the probability density functions of different distributions.
  
### 1. Uniform distribution
This means that we have no prior information about the parameter. All non-negative values of the parameter are equally likely 'a priori'. There are no hyperparameters. This is not a recommended prior to set for any parameter.
  
### 2. Normal distribution $N(\mu,\sigma)$
The normal prior has two hyperparameters, $\mu$ and $\sigma$. The first one defines the location and second one defines the width of the distribution.
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
t   <- seq(0,5,by=0.05)
MU  <- c(0,2)
SIG <- c(2,0.5)
for(k in 1:2){
  mu    <- MU[k]
  sigma <- SIG[k]
  y     <- dnorm(t,mu,sigma)
  graphics::plot(t,y,'l',xlab="x",ylab="p(x)", col="firebrick3", 
     cex.axis=0.6, cex.main = 0.8, bty ="l",
     main = bquote("N(" ~.(mu)~ ","~.(sigma)~")"))
}

```

### 3. Student-t$(\nu)$ distribution
In the Student-t distribution, the hyperparameter $\nu$ that defines the degrees of freedom. The case $\nu=1$ corresponds to the Cauchy-distribution and $\nu \rightarrow \infty$ to the normal distribution with $\mu=0$ and $\sigma=1$. 
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
t   <- seq(0,3,by=0.01)
NU  <- c(1,40)
for(k in 1:2){
  nu    <- NU[k]
  y     <- dt(t,df=nu)
  graphics::plot(t,y,'l',xlab="x",ylab="p(x)", col="firebrick3", 
     cex.axis=0.6, cex.main = 0.8, bty ="l",
     main = bquote("Student-t("~.(nu)~")"))
}

```

### 4. Gamma$(\alpha,\beta)$-distribution
The gamma distribution has two hyperparameters, the shape parameter $\alpha$ and the rate parameter $\beta$. It has the property that Gamma$(1,\beta)$ is the exponential distribution with rate parameter $\beta$.
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
t <- seq(0,5,by=0.01)
A <- c(1,5)
B <- c(3,3)
for(k in 1:2){
  alpha <- A[k]
  beta  <- B[k]
  y     <- dgamma(t,shape=alpha,rate=beta)
  graphics::plot(t,y,'l',xlab="x",ylab="p(x)", col="firebrick3", 
     cex.axis=0.6, cex.main = 0.8, bty ="l",
     main = bquote("Gamma(" ~.(alpha)~ ","~.(beta)~")"))
}
```

### 5. Inv-Gamma$(\alpha,\beta)$ distribution
The inverse-gamma distribution also has the hyperparameters $\alpha$ and $\beta$. Its mode is at $\frac{\beta}{\alpha+1}$. Using an alternative parametrization, it is also called the scaled inverse $\chi^2$-distribution. The connection is
$$
\text{Scaled-inv-}\chi^2(\nu,\tau^2) = \text{Inv-Gamma} \left(\frac{\nu}{2}, \frac{\nu \tau^2}{2} \right)
$$
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
#t <- seq(0,5,by=0.01)
#A <- c(3,3)
#B <- c(3,5)
#for(k in 1:2){
#  alpha <- A[k]
#  beta  <- B[k]
#  y     <- invgamma::dinvgamma(t,shape=alpha,rate=beta)
#  graphics::plot(t,y,'l',xlab="x",ylab="p(x)", col="firebrick3", 
#     cex.axis=0.6, cex.main = 0.8, bty ="l",
#     main = bquote("Inv-Gamma(" ~.(alpha)~ ","~.(beta)~")"))
#}
```

### 6. Log-Normal$(\mu,\sigma)$ distribution
The log-normal distribution is defined so that
$$
\theta \sim \text{LogNormal}(\mu,\sigma)  \hspace{1cm} \Longrightarrow \hspace{1cm}  \log(\theta) \sim N(\mu,\sigma) 
$$
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
t <- seq(0,5,by=0.01)
M <- c(0,0)
S <- c(1,0.25)
for(k in 1:2){
  mlog  <- M[k]
  slog  <- S[k]
  y     <- stats::dlnorm(t,meanlog=mlog, sdlog=slog)
  graphics::plot(t,y,'l',xlab="x",ylab="p(x)", col="firebrick3", 
     cex.axis=0.6, cex.main = 0.8, bty ="l",
     main = bquote("Log-Normal(" ~.(mlog)~ ","~.(slog)~")"))
}
```

### 7. Beta$(\alpha,\beta)$ distribution
The support of the beta distribution is $[0,1]$ and it can be used to represent various distributions on this interval.
  
```{r, fig.show='hold', echo = FALSE, fig.width = 3.45, fig.height = 2.6}
graphics::par(mar=2*c(2,2,1,1))
YMAX <- c(3,3)
LL <- c("topleft", "top")
for(idx in c(1:2)){
  if(idx==1){
    A <- c(4,4,4)
    B <- c(1,2,4)
  }else{
    A <- 0.1*c(5,1)
    B <- 0.1*c(5,1)
  }
  t  <- seq(0, 1, by = 0.001)
  Y  <- matrix(0, length(t), length(A))
  cols <- c("#E41A1C", "#377EB8", "#4DAF4A")
  graphics::plot(c(0,1), c(0,YMAX[idx]), xlab = "x", ylab = "p(x)", col = "white",
                                  cex.axis=0.6, cex.main = 0.8, bty ="l")
  for(j in 1:length(A)){
    y <- dbeta(t, shape1 = A[j], shape2 = B[j])
    graphics::lines(t, y, col = cols[j], lwd = 2) 
  }
  LEG <- paste("Beta(", A, ", ", B, ")", sep = "")
  graphics::legend(LL[idx], legend = LEG, inset = 0.05, 
                 box.col = "black", lty = 1, col = cols, lwd = 2,
                 cex = 0.7)
}
```

## Transforms
In the usual case, the prior for a parameter $X$ is defined to be the distribution $p_{X}(x)$, which is one of the distiributions listed above. However, in some cases we may want to define the prior so that $f(X) \sim p_{X}$, where $f$ is an invertible mapping. In this case, the log Jacobian of the transform is utomatically added to the logarithm of the target density inside the Stan model. The user does not need to worry about this, and the can specify the prior transformation as explained below. The supported transformations and the corresponding log Jacobians are listed in the below table.

```{r, echo = FALSE, results = 'asis'}
transforms <- c("scaling", "square", "exponentiation")
f <- c("$c \\cdot x$", "$x^2$", "$\\exp(x)$")
J <- c("constant","$\\log|2y|$","$y$")
T <- cbind(transforms, f, J)
colnames(T) <- c(" ", "$f(x)$", "$\\log \\left| \\frac{\\partial}{\\partial y} f(y) \\right|$") 
knitr::kable(T)
```

[insert how to define the transforms in the lgp function]

## Calibrating the magnitude priors
Let $\bf{f}$ = $[f_1, \ldots, f_n]^\top$ be a vector containing the signal values at $n$ measurement points. If the model has $D$ components, the additive GP prior then specifies the prior distribution of these function values as the multivariate normal
$$
\bf{f} \sim \mathcal{N}\left(\bf{0}, \bf{K}\right), 
$$
where $\bf{K} = \alpha_1^2 \bf{K}_1 + \ldots + \alpha_D^2 \bf{K}_D$. This means that the magnitude parameters $\alpha_j^2$, $j = 1, \ldots, D$ of the additive components define how large variations the different signal components can experience. Their priors should therefore be specified according to beliefs about the variance of different components. On the other hand, at the same time we would like our entire prior formulation to reflect our beliefs about the observed measurements $\bf{y}$. 

### Gaussian observation model
In the Gaussian observation model we assume 
$$
y_i = f_i + \epsilon_i,
$$
where $\epsilon_i \sim \mathcal{N}(0, \sigma_n^2)$ independently for all $i=1,\ldots, n$. 

```{r, results = "hide"}
set.seed(123)
library(lgpr)

# Define a prior for the magnitude parameters
my_prior <- list()
#my_prior$magnitude$idAge         <- list(type="student_t", nu=4)
#my_prior$magnitude$sharedAge     <- list(type="student_t", nu=20)
#my_prior$magnitude$diseaseAge    <- list(type="student_t", nu=20)
#my_prior$magnitude$continuous    <- list(type="normal", mu=0, sigma=1)
#my_prior$magnitude$categorical   <- list(type="uniform")
#my_prior$magnitude$offset        <- list(type="gamma", alpha=5, beta=2)
my_prior$sigma_n                 <- list(type="normal", mu=5, sigma=0.5, transform = "square")

# Draw samples from the prior
#out <- validatePrior(prior = my_prior,
#                      n_diseaseAge = 0, 
#                      n_categorical = 2)

```

We can graphics::plot a kernel density estimates of the prior distributions using the drawn samples.
```{r, fig.show='hold', fig.width = 7, fig.height = 4, fig.align="center"}
# Visualize the prior samples
#graphics::plotMagnitudes(out$lgp_fit, noisepar = "sigma_n")

```

```{r, fig.show='hold', fig.width = 7, fig.height = 7/3, fig.align="center"}
# Visualize the realizations of f and y
#validatePriorsgraphics::plot(out, opacity = 0.1)
```


### Non-Gaussian observation model
